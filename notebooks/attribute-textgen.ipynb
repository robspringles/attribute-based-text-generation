{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute-based Text Generation from Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import zeros, concatenate, asarray\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notification JavaScript snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def browser_alert(message):\n",
    "    display(HTML('<script type=\"text/javascript\">alert(\"' + message + '\");</script>'))\n",
    "    \n",
    "def browser_notify(message):\n",
    "    display(HTML('<script type=\"text/javascript\">var notification=new Notification(\"' + \\\n",
    "                 'Jupyter Notification\",{icon:\"http://blog.jupyter.org/content/' + \\\n",
    "                 'images/2015/02/jupyter-sq-text.png\",body:\"' + message + \\\n",
    "                 '\"});</script>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_notify(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/home/v2john/attr-reviews-dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_set_path = dataset_path + \"dev.txt\"\n",
    "train_set_path = dataset_path + \"train.txt\"\n",
    "test_set_path = dataset_path + \"test.txt\"\n",
    "vocab_path = dataset_path + \"vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab_index(vocab_file_path):\n",
    "    \n",
    "    vocab_index = dict()\n",
    "    current_index = 0\n",
    "    \n",
    "    with open(vocab_file_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word = line.split()[0]\n",
    "            vocab_index[word] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "    return vocab_index\n",
    "\n",
    "\n",
    "def build_user_and_product_indices(review_file_path):\n",
    "    \n",
    "    user_index = dict()\n",
    "    product_index = dict()\n",
    "    \n",
    "    current_user_index = 0\n",
    "    current_product_index = 0\n",
    "    \n",
    "    with open(review_file_path) as review_file:\n",
    "        for line in review_file:\n",
    "            split_line = line.split()\n",
    "            user_id = split_line[0]\n",
    "            product_id = split_line[1]\n",
    "            \n",
    "            if user_id not in user_index:\n",
    "                user_index[user_id] = current_user_index\n",
    "                current_user_index += 1\n",
    "                \n",
    "            if product_id not in product_index:\n",
    "                product_index[product_id] = current_product_index\n",
    "                current_product_index += 1\n",
    "                \n",
    "    return user_index, product_index\n",
    "\n",
    "\n",
    "def build_rating_index():\n",
    "    \n",
    "    rating_index = dict()\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        rating_index[(i + 1) * 1.0] = i\n",
    "        \n",
    "    return rating_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index = build_vocab_index(vocab_path)\n",
    "num_words = len(vocab_index)\n",
    "print(str(num_words) + \" terms in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index, product_index = build_user_and_product_indices(dev_set_path)\n",
    "num_users = len(user_index)\n",
    "num_products = len(product_index)\n",
    "print(str(num_users) + \" distinct users\")\n",
    "print(str(num_products) + \" distinct products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_index = build_rating_index()\n",
    "num_ratings = len(rating_index)\n",
    "print(str(num_ratings) + \" distinct ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_notify(\"Indices built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert build attribute and text-embedding versions of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_term_matrix(term_index, term, term_rows, term_cols, term_data, index):\n",
    "        \n",
    "        \n",
    "    term_data.append(1)\n",
    "    term_rows.append(index)\n",
    "    term_cols.append(term_index[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_sequence(review, vocab_index):\n",
    "    \n",
    "    review_sequence = list()\n",
    "    tokens = word_tokenize(review)\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            word_index = update_term_matrix(vocab_index, num_words, str(token))\n",
    "            review_sequence.append(word_index)\n",
    "        except KeyError:\n",
    "            pass  # skip missing keys\n",
    "    \n",
    "    return asarray(review_sequence)\n",
    "\n",
    "def get_attribute_vectors_and_text_embedding_sequences(review_file_path):\n",
    "\n",
    "    x_train_att = list()\n",
    "    x_train_rev = list()\n",
    "    i = 0\n",
    "    user_rows = list()\n",
    "    user_cols = list()\n",
    "    user_data = list()\n",
    "    product_rows = list()\n",
    "    product_cols = list()\n",
    "    product_data = list()\n",
    "    rating_rows = list()\n",
    "    rating_cols = list()\n",
    "    rating_data = list()\n",
    "    \n",
    "    with open(review_file_path) as review_file:\n",
    "        for line in review_file:\n",
    "            split_line = line.split(\"\\t\")\n",
    "            user_id = split_line[0]\n",
    "            product_id = split_line[1]\n",
    "            rating = float(split_line[2])\n",
    "            review_text = split_line[3]\n",
    "            \n",
    "            update_term_matrix(user_index, user_id, user_rows, user_cols, user_data, i),\n",
    "            update_term_matrix(product_index, product_id, product_rows, product_cols, product_data, i),\n",
    "            update_term_matrix(rating_index, rating, rating_rows, rating_cols, rating_data, i)\n",
    "            \n",
    "            x_train_rev.append(get_review_sequence(review_text, vocab_index))\n",
    "            \n",
    "    return asarray(x_train_att), asarray(x_train_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_att, x_train_rev = get_attribute_vectors_and_text_embedding_sequences(dev_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_att.shape, x_train_att[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_rev.shape, x_train_rev[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatfiles_path = \"/home/v2john/text-gen-flatfiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save vectors\n",
    "\n",
    "# with open(flatfiles_path + 'x_train_att.pkl', 'wb') as x_train_att_file:\n",
    "#     pickle.dump(x_train_att, x_train_att_file)\n",
    "\n",
    "# with open(flatfiles_path + 'x_train_rev.pkl', 'wb') as x_train_rev_file:\n",
    "#     pickle.dump(x_train_rev, x_train_rev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restore vectors\n",
    "\n",
    "with open(flatfiles_path + 'x_train_att.pkl', 'rb') as x_train_att_file:\n",
    "    x_train_att = pickle.load(x_train_att_file)\n",
    "    \n",
    "with open(flatfiles_path + 'x_train_rev.pkl', 'rb') as x_train_rev_file:\n",
    "    x_train_rev = pickle.load(x_train_rev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_att.shape, x_train_rev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, RepeatVector, LSTM, Conv1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_rev = pad_sequences(x_train_rev, maxlen=10, padding='post', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=x_train_att[0].shape)\n",
    "outputs = Input(shape=x_train_rev[0].shape)\n",
    "\n",
    "x = Dense(100, activation='relu')(inputs)\n",
    "print(x)\n",
    "x = Dense(1, activation='relu')(x)\n",
    "print(x)\n",
    "x = RepeatVector(10)(x)\n",
    "print(x)\n",
    "x = LSTM(1, return_sequences=True)(x)\n",
    "print(x)\n",
    "x = LSTM(1, return_sequences=True)(x)\n",
    "print(x)\n",
    "x = LSTM(1, return_sequences=True)(x)\n",
    "print(x)\n",
    "\n",
    "model = Model(inputs, x)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='kullback_leibler_divergence',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_att, x_train_rev, batch_size=32, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_att[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(asarray([[1, 9,  4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
