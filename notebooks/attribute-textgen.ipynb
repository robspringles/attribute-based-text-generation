{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute-based Text Generation from Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import zeros, concatenate, asarray\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notification JavaScript snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def browser_alert(message):\n",
    "    display(HTML('<script type=\"text/javascript\">alert(\"' + message + '\");</script>'))\n",
    "    \n",
    "def browser_notify(message):\n",
    "    display(HTML('<script type=\"text/javascript\">var notification=new Notification(\"' + \\\n",
    "                 'Jupyter Notification\",{icon:\"http://blog.jupyter.org/content/' + \\\n",
    "                 'images/2015/02/jupyter-sq-text.png\",body:\"' + message + \\\n",
    "                 '\"});</script>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_notify(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/home/v2john/attr-reviews-dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_set_path = dataset_path + \"dev.txt\"\n",
    "train_set_path = dataset_path + \"train.txt\"\n",
    "test_set_path = dataset_path + \"test.txt\"\n",
    "vocab_path = dataset_path + \"vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab_index(vocab_file_path):\n",
    "    \n",
    "    vocab_index = dict()\n",
    "    current_index = 0\n",
    "    \n",
    "    with open(vocab_file_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word = line.split()[0]\n",
    "            vocab_index[word] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "    return vocab_index\n",
    "\n",
    "\n",
    "def build_user_and_product_indices(review_file_path):\n",
    "    \n",
    "    user_index = dict()\n",
    "    product_index = dict()\n",
    "    \n",
    "    current_user_index = 0\n",
    "    current_product_index = 0\n",
    "    \n",
    "    with open(review_file_path) as review_file:\n",
    "        for line in review_file:\n",
    "            split_line = line.split()\n",
    "            user_id = split_line[0]\n",
    "            product_id = split_line[1]\n",
    "            \n",
    "            if user_id not in user_index:\n",
    "                user_index[user_id] = current_user_index\n",
    "                current_user_index += 1\n",
    "                \n",
    "            if product_id not in product_index:\n",
    "                product_index[product_id] = current_product_index\n",
    "                current_product_index += 1\n",
    "                \n",
    "    return user_index, product_index\n",
    "\n",
    "\n",
    "def build_rating_index():\n",
    "    \n",
    "    rating_index = dict()\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        rating_index[(i + 1) * 1.0] = i\n",
    "        \n",
    "    return rating_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index = build_vocab_index(vocab_path)\n",
    "print(str(len(vocab_index)) + \" terms in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index, product_index = build_user_and_product_indices(dev_set_path)\n",
    "print(str(len(user_index)) + \" distinct users\")\n",
    "print(str(len(product_index)) + \" distinct products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_index = build_rating_index()\n",
    "print(str(len(rating_index)) + \" distinct ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_notify(\"Indices built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert build attribute and text-embedding versions of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_one_hot_vector_embedding(term_index, term):\n",
    "    return asarray([term_index[term]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_sequence(review, vocab_index):\n",
    "    \n",
    "    review_sequence = list()\n",
    "    \n",
    "    tokens = word_tokenize(review)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            word_index = get_one_hot_vector_embedding(vocab_index, str(token))\n",
    "            review_sequence.append(word_index)\n",
    "        except KeyError as ke:\n",
    "            pass  # skip missing keys\n",
    "    \n",
    "    return asarray(review_sequence)\n",
    "\n",
    "def get_attribute_vectors_and_text_embedding_sequences(review_file_path):\n",
    "\n",
    "    x_train_att = list()\n",
    "    x_train_rev = list()\n",
    "    i = 0\n",
    "    \n",
    "    with open(review_file_path) as review_file:\n",
    "        for line in review_file:\n",
    "            split_line = line.split(\"\\t\")\n",
    "            user_id = split_line[0]\n",
    "            product_id = split_line[1]\n",
    "            rating = float(split_line[2])\n",
    "            review_text = split_line[3]\n",
    "            \n",
    "            attribute_vector = concatenate((get_one_hot_vector_embedding(user_index, user_id), \n",
    "                                            get_one_hot_vector_embedding(product_index, product_id),\n",
    "                                            get_one_hot_vector_embedding(rating_index, rating)))\n",
    "            \n",
    "            x_train_att.append(attribute_vector)\n",
    "            x_train_rev.append(get_review_sequence(review_text, vocab_index))\n",
    "            \n",
    "#             i += 1\n",
    "#             if i == 10:\n",
    "#                 break\n",
    "            \n",
    "    return asarray(x_train_att), asarray(x_train_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_att, x_train_rev = get_attribute_vectors_and_text_embedding_sequences(dev_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_att.shape, x_train_att[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rev.shape, x_train_rev[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatfiles_path = \"/home/v2john/text-gen-flatfiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectors\n",
    "\n",
    "with open(flatfiles_path + 'x_train_att.pkl', 'wb') as x_train_att_file:\n",
    "    pickle.dump(x_train_att, x_train_att_file)\n",
    "\n",
    "with open(flatfiles_path + 'x_train_rev.pkl', 'wb') as x_train_rev_file:\n",
    "    pickle.dump(x_train_rev, x_train_rev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Restore vectors\n",
    "\n",
    "# with open(flatfiles_path + 'x_train_att.pkl', 'rb') as x_train_att_file:\n",
    "#     x_train_att = pickle.load(x_train_att_file)\n",
    "    \n",
    "# with open(flatfiles_path + 'x_train_rev.pkl', 'rb') as x_train_rev_file:\n",
    "#     x_train_rev = pickle.load(x_train_rev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, RepeatVector, LSTM, Conv1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=x_train_att[0].shape)\n",
    "print(x_train_att[0])\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "print(x)\n",
    "x = Dense(1, activation='relu')(inputs)\n",
    "print(x)\n",
    "predictions = RepeatVector(10)(x)\n",
    "print(predictions)\n",
    "# predictions = Conv1D(64, activation='relu', kernel_size=3)(x)\n",
    "# print(predictions)\n",
    "\n",
    "model = Model(inputs, predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_att, x_train_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
